{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to replace 'path/to/credentials.json' with the actual path to your Google Sheets API credentials file.\n",
    "\n",
    "This program uses the requests library to send HTTP requests to the website, the BeautifulSoup library to parse the HTML content and extract the required information, and the gspread library to access and modify the Google Sheet. The program defines two functions: scrape_website to scrape the website and extract the information, and write_to_google_sheet to write the extracted information to the Google Sheet. The extracted information is then written to the Google Sheet specified in the URL.\n",
    "\n",
    "Please note that the website structure may change, requiring updates to the program. Additionally, ensure that you have enabled the Google Sheets API, created the necessary credentials, and have the correct permissions to access and modify the Google Sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Applications/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.11/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.11/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: beautifulsoup4 in /Applications/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Applications/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: gspread in /Applications/anaconda3/lib/python3.11/site-packages (5.12.3)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in /Applications/anaconda3/lib/python3.11/site-packages (from gspread) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /Applications/anaconda3/lib/python3.11/site-packages (from gspread) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth>=1.12.0->gspread) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth>=1.12.0->gspread) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth>=1.12.0->gspread) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Applications/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2023.11.17)\n",
      "Requirement already satisfied: oauth2client in /Applications/anaconda3/lib/python3.11/site-packages (4.1.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /Applications/anaconda3/lib/python3.11/site-packages (from oauth2client) (0.22.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /Applications/anaconda3/lib/python3.11/site-packages (from oauth2client) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /Applications/anaconda3/lib/python3.11/site-packages (from oauth2client) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Applications/anaconda3/lib/python3.11/site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in /Applications/anaconda3/lib/python3.11/site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Applications/anaconda3/lib/python3.11/site-packages (from httplib2>=0.9.1->oauth2client) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install gspread\n",
    "!pip install oauth2client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the function to scrape the website and extract the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(url):\n",
    "    # Make an HTTP GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the response\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all the cards on the website\n",
    "        cards = soup.find_all('div', class_='card')\n",
    "\n",
    "        # Initialize lists to store the extracted information\n",
    "        titles = []\n",
    "        urls = []\n",
    "        # founding_years = []\n",
    "        # total_funding = []\n",
    "\n",
    "        # Extract the URLs, founding year, and total funding from each card\n",
    "        for card in cards:\n",
    "            # Extract the title\n",
    "            title = card.find('h2', class_='text-lg font-bold').text\n",
    "            titles.append(title)\n",
    "            \n",
    "            # Extract the URL\n",
    "            url = card.find('a', class_='false max-w-full overflow-hidden text-clip leading-5 text-purple-400 line-clamp-1').text\n",
    "            urls.append(url)\n",
    "\n",
    "            # # Extract the founding year\n",
    "            # founding_year = card.find('span', class_='founding-year').text\n",
    "            # founding_years.append(founding_year)\n",
    "\n",
    "            # # Extract the total funding\n",
    "            # funding = card.find('span', class_='total-funding').text\n",
    "            # total_funding.append(funding)\n",
    "\n",
    "        # Return the extracted information\n",
    "        return titles, urls #, founding_years, total_funding\n",
    "\n",
    "    else:\n",
    "        # The request was not successful, return None\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(titles, urls, founding_years, total_funding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define the function to write the extracted information to the Google Sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_google_sheet(urls, titles): #, founding_years, total_funding):\n",
    "    # Define the scope and credentials\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('/Users/robertford/_Projects/ScraperGPT/affable-bastion-324718-46b5767bba57.json', scope)\n",
    "\n",
    "    # Authenticate using the credentials\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    # Access the Google Sheet\n",
    "    spreadsheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1-DrchRkuG5j-gsa2kI1sbw3cfamq2me7KkTrWRgSv2o/edit#gid=0')\n",
    "\n",
    "    # Select the first sheet\n",
    "    sheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "    # Clear the existing data in the sheet\n",
    "    sheet.clear()\n",
    "\n",
    "    # Write the headers to the sheet\n",
    "    sheet.update_cell(1, 1, 'titles')\n",
    "    sheet.update_cell(1, 2, 'urls')\n",
    "    # sheet.update_cell(1, 3, 'founding_year')\n",
    "    # sheet.update_cell(1, 4, 'total_funding')\n",
    "\n",
    "    # Write the extracted information to the sheet\n",
    "    for i in range(len(urls)):\n",
    "        sheet.update_cell(i+2, 1, urls[i])\n",
    "        sheet.update_cell(i+2, 2, titles[i])\n",
    "        # sheet.update_cell(i+2, 3, founding_years[i])\n",
    "        # sheet.update_cell(i+2, 4, total_funding[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Call the function to scrape the website and write the extracted information to the Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = 'https://mad.firstmark.com/card'\n",
    "# titles, urls, founding_years, total_funding = scrape_website(website_url)\n",
    "titles, urls = scrape_website(website_url)\n",
    "# write_to_google_sheet(titles, urls, founding_years, total_funding)\n",
    "write_to_google_sheet(titles, urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
